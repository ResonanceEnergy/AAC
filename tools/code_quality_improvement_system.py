"""
AAC Code Quality Improvement System
====================================

Automated code quality improvements and refactoring.
Addresses code quality issues: large files, tight coupling, formatting, duplicates, limited tests.

Features:
- Automated code analysis and refactoring
- Duplicate code detection and consolidation
- Large file decomposition
- Coupling analysis and decoupling
- Test coverage improvement
- Code formatting and linting
"""

import asyncio
import logging
import ast
import re
import os
import hashlib
from typing import Dict, List, Any, Optional, Tuple, Set
from pathlib import Path
from collections import defaultdict, Counter
import inspect
import importlib.util

from shared.audit_logger import AuditLogger
from shared.communication import CommunicationFramework

logger = logging.getLogger(__name__)


class CodeQualityAnalyzer:
    """
    Analyzes code quality issues across the codebase.
    """

    def __init__(self, audit_logger: AuditLogger, communication: CommunicationFramework):
        self.audit_logger = audit_logger
        self.communication = communication

        # Quality thresholds
        self.thresholds = {
            'max_file_lines': 500,
            'max_function_lines': 50,
            'max_class_lines': 200,
            'min_test_coverage': 80,
            'max_coupling_score': 10,
            'max_duplicate_lines': 20
        }

        # Analysis results
        self.analysis_results = {}

    async def run_full_quality_analysis(self) -> Dict[str, Any]:
        """Run complete code quality analysis"""
        logger.info("Starting full code quality analysis...")

        results = {
            'timestamp': asyncio.get_event_loop().time(),
            'files_analyzed': 0,
            'issues_found': 0,
            'quality_score': 0,
            'categories': {}
        }

        try:
            # Analyze all Python files
            python_files = list(Path('.').rglob('*.py'))
            results['files_analyzed'] = len(python_files)

            # File size analysis
            results['categories']['file_sizes'] = await self._analyze_file_sizes(python_files)

            # Code complexity analysis
            results['categories']['complexity'] = await self._analyze_code_complexity(python_files)

            # Duplicate code detection
            results['categories']['duplicates'] = await self._detect_duplicate_code(python_files)

            # Coupling analysis
            results['categories']['coupling'] = await self._analyze_coupling(python_files)

            # Test coverage analysis
            results['categories']['testing'] = await self._analyze_test_coverage()

            # Code formatting analysis
            results['categories']['formatting'] = await self._analyze_code_formatting(python_files)

            # Calculate overall quality score
            results['issues_found'] = sum(cat.get('issues', 0) for cat in results['categories'].values())
            results['quality_score'] = await self._calculate_quality_score(results)

        except Exception as e:
            logger.error(f"Quality analysis failed: {e}")
            results['error'] = str(e)

        self.analysis_results = results
        await self._log_analysis_results(results)

        return results

    async def _analyze_file_sizes(self, files: List[Path]) -> Dict[str, Any]:
        """Analyze file sizes and identify large files"""
        large_files = []
        total_lines = 0

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    line_count = len(lines)
                    total_lines += line_count

                    if line_count > self.thresholds['max_file_lines']:
                        large_files.append({
                            'file': str(file_path),
                            'lines': line_count,
                            'functions': len([l for l in lines if l.strip().startswith('def ')]),
                            'classes': len([l for l in lines if l.strip().startswith('class ')])
                        })

            except Exception as e:
                logger.warning(f"Could not analyze {file_path}: {e}")

        return {
            'large_files': large_files,
            'total_files': len(files),
            'total_lines': total_lines,
            'avg_lines_per_file': total_lines / len(files) if files else 0,
            'issues': len(large_files)
        }

    async def _analyze_code_complexity(self, files: List[Path]) -> Dict[str, Any]:
        """Analyze code complexity metrics"""
        complex_functions = []
        complex_classes = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        line_count = node.end_lineno - node.lineno
                        if line_count > self.thresholds['max_function_lines']:
                            complex_functions.append({
                                'file': str(file_path),
                                'function': node.name,
                                'lines': line_count,
                                'complexity': self._estimate_complexity(node)
                            })

                    elif isinstance(node, ast.ClassDef):
                        line_count = node.end_lineno - node.lineno
                        if line_count > self.thresholds['max_class_lines']:
                            complex_classes.append({
                                'file': str(file_path),
                                'class': node.name,
                                'lines': line_count,
                                'methods': len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                            })

            except Exception as e:
                logger.warning(f"Could not analyze complexity for {file_path}: {e}")

        return {
            'complex_functions': complex_functions,
            'complex_classes': complex_classes,
            'issues': len(complex_functions) + len(complex_classes)
        }

    def _estimate_complexity(self, node: ast.FunctionDef) -> int:
        """Estimate cyclomatic complexity of a function"""
        complexity = 1  # Base complexity

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.With)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
            elif isinstance(child, ast.Try):
                complexity += len(child.handlers)

        return complexity

    async def _detect_duplicate_code(self, files: List[Path]) -> Dict[str, Any]:
        """Detect duplicate code blocks"""
        code_blocks = defaultdict(list)
        duplicates = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Extract function and class bodies
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                        # Get source code for the node
                        start_line = node.lineno - 1
                        end_line = node.end_lineno
                        lines = content.split('\n')[start_line:end_line]
                        code_block = '\n'.join(lines)

                        # Create hash for comparison
                        code_hash = hashlib.md5(code_block.encode()).hexdigest()
                        code_blocks[code_hash].append({
                            'file': str(file_path),
                            'name': node.name,
                            'type': 'function' if isinstance(node, ast.FunctionDef) else 'class',
                            'lines': len(lines)
                        })

            except Exception as e:
                logger.warning(f"Could not analyze duplicates in {file_path}: {e}")

        # Find duplicates
        for code_hash, occurrences in code_blocks.items():
            if len(occurrences) > 1:
                duplicates.append({
                    'hash': code_hash,
                    'occurrences': occurrences,
                    'total_occurrences': len(occurrences)
                })

        return {
            'duplicates': duplicates,
            'total_duplicate_blocks': len(duplicates),
            'issues': len(duplicates)
        }

    async def _analyze_coupling(self, files: List[Path]) -> Dict[str, Any]:
        """Analyze coupling between modules"""
        imports = defaultdict(set)
        high_coupling = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Extract imports
                tree = ast.parse(content)

                module_imports = set()
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            module_imports.add(alias.name.split('.')[0])
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            module_imports.add(node.module.split('.')[0])

                imports[str(file_path)] = module_imports

            except Exception as e:
                logger.warning(f"Could not analyze coupling for {file_path}: {e}")

        # Calculate coupling scores
        for file_path, file_imports in imports.items():
            coupling_score = len(file_imports)
            if coupling_score > self.thresholds['max_coupling_score']:
                high_coupling.append({
                    'file': file_path,
                    'imports': list(file_imports),
                    'coupling_score': coupling_score
                })

        return {
            'high_coupling_files': high_coupling,
            'total_modules': len(imports),
            'issues': len(high_coupling)
        }

    async def _analyze_test_coverage(self) -> Dict[str, Any]:
        """Analyze test coverage"""
        # Mock test coverage analysis
        coverage_data = {
            'overall_coverage': 65.5,
            'files_with_tests': 45,
            'files_without_tests': 23,
            'uncovered_lines': 1250
        }

        issues = 1 if coverage_data['overall_coverage'] < self.thresholds['min_test_coverage'] else 0

        return {
            'coverage': coverage_data,
            'issues': issues
        }

    async def _analyze_code_formatting(self, files: List[Path]) -> Dict[str, Any]:
        """Analyze code formatting issues"""
        formatting_issues = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                issues = []

                # Check for mixed tabs/spaces
                if '\t' in content and '    ' in content:
                    issues.append('mixed_tabs_spaces')

                # Check line length
                long_lines = [i+1 for i, line in enumerate(content.split('\n')) if len(line) > 100]
                if long_lines:
                    issues.append(f'long_lines: {len(long_lines)} lines > 100 chars')

                # Check trailing whitespace
                trailing_ws = [i+1 for i, line in enumerate(content.split('\n')) if line.rstrip() != line]
                if trailing_ws:
                    issues.append(f'trailing_whitespace: {len(trailing_ws)} lines')

                if issues:
                    formatting_issues.append({
                        'file': str(file_path),
                        'issues': issues
                    })

            except Exception as e:
                logger.warning(f"Could not analyze formatting for {file_path}: {e}")

        return {
            'formatting_issues': formatting_issues,
            'issues': len(formatting_issues)
        }

    async def _calculate_quality_score(self, results: Dict[str, Any]) -> float:
        """Calculate overall quality score (0-100)"""
        total_issues = results['issues_found']
        files_analyzed = results['files_analyzed']

        if files_analyzed == 0:
            return 0.0

        # Base score starts at 100, deduct points for issues
        base_score = 100.0
        deduction_per_issue = 2.0  # 2 points per issue

        score = max(0.0, base_score - (total_issues * deduction_per_issue))

        return round(score, 1)

    async def _log_analysis_results(self, results: Dict[str, Any]):
        """Log analysis results"""
        logger.info(f"Code quality analysis complete: {results['quality_score']}% score, {results['issues_found']} issues")


class CodeQualityImprover:
    """
    Automatically improves code quality issues.
    """

    def __init__(self, analyzer: CodeQualityAnalyzer):
        self.analyzer = analyzer

    async def apply_automated_fixes(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Apply automated code quality fixes"""
        logger.info("Applying automated code quality fixes...")

        fixes_applied = {
            'formatting_fixes': 0,
            'duplicate_consolidation': 0,
            'refactoring_suggestions': 0,
            'test_generation': 0
        }

        try:
            # Fix formatting issues
            formatting_fixes = await self._fix_formatting_issues(analysis_results)
            fixes_applied['formatting_fixes'] = formatting_fixes

            # Generate duplicate consolidation suggestions
            duplicate_fixes = await self._suggest_duplicate_consolidation(analysis_results)
            fixes_applied['duplicate_consolidation'] = duplicate_fixes

            # Generate refactoring suggestions
            refactoring_suggestions = await self._generate_refactoring_suggestions(analysis_results)
            fixes_applied['refactoring_suggestions'] = refactoring_suggestions

            # Generate missing tests
            test_generation = await self._generate_missing_tests(analysis_results)
            fixes_applied['test_generation'] = test_generation

        except Exception as e:
            logger.error(f"Automated fixes failed: {e}")

        return fixes_applied

    async def _fix_formatting_issues(self, analysis_results: Dict[str, Any]) -> int:
        """Fix basic formatting issues"""
        formatting_data = analysis_results['categories']['formatting']
        fixes_applied = 0

        for issue in formatting_data['formatting_issues']:
            file_path = issue['file']

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                original_content = content

                # Fix trailing whitespace
                lines = content.split('\n')
                lines = [line.rstrip() for line in lines]
                content = '\n'.join(lines)

                # Fix mixed tabs/spaces (convert tabs to spaces)
                content = content.expandtabs(4)

                # Write back if changed
                if content != original_content:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    fixes_applied += 1
                    logger.info(f"Fixed formatting in {file_path}")

            except Exception as e:
                logger.warning(f"Could not fix formatting for {file_path}: {e}")

        return fixes_applied

    async def _suggest_duplicate_consolidation(self, analysis_results: Dict[str, Any]) -> int:
        """Generate suggestions for consolidating duplicate code"""
        duplicates_data = analysis_results['categories']['duplicates']
        suggestions = 0

        # Create a report of duplicate code blocks
        report_path = Path("code_quality_reports/duplicate_code_report.txt")
        report_path.parent.mkdir(exist_ok=True)

        with open(report_path, 'w') as f:
            f.write("Duplicate Code Report\n")
            f.write("=" * 50 + "\n\n")

            for duplicate in duplicates_data['duplicates']:
                f.write(f"Duplicate Code Block (Hash: {duplicate['hash'][:8]})\n")
                f.write(f"Occurrences: {duplicate['total_occurrences']}\n")
                f.write("Files:\n")

                for occurrence in duplicate['occurrences']:
                    f.write(f"  - {occurrence['file']}: {occurrence['type']} {occurrence['name']} ({occurrence['lines']} lines)\n")

                f.write("\nRecommendation: Extract to shared utility function\n\n")
                suggestions += 1

        logger.info(f"Generated {suggestions} duplicate consolidation suggestions")
        return suggestions

    async def _generate_refactoring_suggestions(self, analysis_results: Dict[str, Any]) -> int:
        """Generate refactoring suggestions for large/complex code"""
        suggestions = 0

        # Large files
        large_files = analysis_results['categories']['file_sizes']['large_files']
        for file_info in large_files:
            await self._suggest_file_refactoring(file_info)
            suggestions += 1

        # Complex functions
        complex_functions = analysis_results['categories']['complexity']['complex_functions']
        for func_info in complex_functions:
            await self._suggest_function_refactoring(func_info)
            suggestions += 1

        return suggestions

    async def _suggest_file_refactoring(self, file_info: Dict[str, Any]):
        """Suggest refactoring for large files"""
        report_path = Path(f"code_quality_reports/refactoring_{Path(file_info['file']).stem}.txt")
        report_path.parent.mkdir(exist_ok=True)

        with open(report_path, 'w') as f:
            f.write(f"Refactoring Suggestions for {file_info['file']}\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"File size: {file_info['lines']} lines\n")
            f.write(f"Functions: {file_info['functions']}\n")
            f.write(f"Classes: {file_info['classes']}\n\n")

            f.write("Suggested Refactoring Steps:\n")
            f.write("1. Extract utility functions to separate modules\n")
            f.write("2. Split large classes into smaller, focused classes\n")
            f.write("3. Move configuration to separate config files\n")
            f.write("4. Extract constants to dedicated constants module\n")
            f.write("5. Create factory classes for object creation\n\n")

            f.write("Estimated benefits:\n")
            f.write("- Improved maintainability\n")
            f.write("- Better testability\n")
            f.write("- Reduced coupling\n")
            f.write("- Easier debugging\n")

    async def _suggest_function_refactoring(self, func_info: Dict[str, Any]):
        """Suggest refactoring for complex functions"""
        report_path = Path(f"code_quality_reports/function_refactoring_{func_info['function']}.txt")
        report_path.parent.mkdir(exist_ok=True)

        with open(report_path, 'w') as f:
            f.write(f"Function Refactoring: {func_info['function']}\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"File: {func_info['file']}\n")
            f.write(f"Lines: {func_info['lines']}\n")
            f.write(f"Estimated complexity: {func_info['complexity']}\n\n")

            f.write("Refactoring Suggestions:\n")
            f.write("1. Extract helper functions for complex logic blocks\n")
            f.write("2. Replace conditional chains with polymorphism\n")
            f.write("3. Use early returns to reduce nesting\n")
            f.write("4. Extract configuration to parameters\n")
            f.write("5. Add comprehensive error handling\n\n")

    async def _generate_missing_tests(self, analysis_results: Dict[str, Any]) -> int:
        """Generate basic test templates for files without tests"""
        test_generation = 0

        # Files without tests (mock data)
        files_without_tests = [
            "orchestrator.py",
            "command_center.py",
            "deployment_engine.py"
        ]

        for file_name in files_without_tests:
            if Path(file_name).exists():
                await self._generate_test_template(file_name)
                test_generation += 1

        return test_generation

    async def _generate_test_template(self, file_name: str):
        """Generate a basic test template for a file"""
        test_file = Path(f"tests/test_{Path(file_name).stem}.py")
        test_file.parent.mkdir(exist_ok=True)

        template = f'''"""
Unit tests for {file_name}
"""

import unittest
import sys
import os

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from {Path(file_name).stem} import *


class Test{Path(file_name).stem.title()}(unittest.TestCase):
    """Test cases for {Path(file_name)}"""

    def setUp(self):
        """Set up test fixtures"""
        pass

    def tearDown(self):
        """Clean up test fixtures"""
        pass

    def test_initialization(self):
        """Test basic initialization"""
        # TODO: Implement test
        self.assertTrue(True)  # Placeholder

    def test_main_functionality(self):
        """Test main functionality"""
        # TODO: Implement test
        self.assertTrue(True)  # Placeholder


if __name__ == '__main__':
    unittest.main()
'''

        with open(test_file, 'w') as f:
            f.write(template)

        logger.info(f"Generated test template: {test_file}")


async def run_code_quality_improvement():
    """Run the complete code quality improvement pipeline"""
    print("ðŸ”§ AAC Code Quality Improvement System")
    print("=" * 50)

    # Initialize components (mock)
    from shared.audit_logger import AuditLogger
    from shared.communication import CommunicationFramework

    analyzer = CodeQualityAnalyzer(
        audit_logger=AuditLogger(),
        communication=CommunicationFramework()
    )

    improver = CodeQualityImprover(analyzer)

    # Run analysis
    print("ðŸ“Š Analyzing code quality...")
    analysis_results = await analyzer.run_full_quality_analysis()

    print("\nðŸ“ˆ Analysis Results:")
    print(f"   Files Analyzed: {analysis_results['files_analyzed']}")
    print(f"   Issues Found: {analysis_results['issues_found']}")
    print(f"   Quality Score: {analysis_results['quality_score']}%")
    print("\nðŸ“‹ Category Breakdown:")
    for category, data in analysis_results['categories'].items():
        issues = data.get('issues', 0)
        print(f"   {category}: {issues} issues")

    # Apply automated fixes
    print("\nðŸ”§ Applying automated fixes...")
    fixes_applied = await improver.apply_automated_fixes(analysis_results)

    print("\nâœ… Fixes Applied:")
    print(f"   Formatting fixes: {fixes_applied['formatting_fixes']}")
    print(f"   Duplicate consolidation: {fixes_applied['duplicate_consolidation']}")
    print(f"   Refactoring suggestions: {fixes_applied['refactoring_suggestions']}")
    print(f"   Test generation: {fixes_applied['test_generation']}")

    return {
        'analysis': analysis_results,
        'fixes': fixes_applied
    }


if __name__ == "__main__":
    asyncio.run(run_code_quality_improvement())